{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f26ddd3",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 9: Build a Log Aggregator\n",
    "\n",
    "In this lab, you will create your own log generator, build a command-line utility that scans log files, summarizes their contents, and provides insight into system behavior. Data structures to track log message levels such as `INFO`, `WARNING`, `ERROR`, and `CRITICAL`.\n",
    "\n",
    "This lab reinforces:\n",
    "- File I/O\n",
    "- Pattern recognition (regex)\n",
    "- Dictionaries and counters\n",
    "- Functions and modularity\n",
    "- CLI arguments, logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5ee8a",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Create Log files (20%)\n",
    "Using the the following example log format below create a **python file** that will log errors In a structured tree format \n",
    "\n",
    "You will find examples in the folder called Logs that you can use to build your program.\n",
    "\n",
    "Remember set of logs should have a varied levels of log entries (`INFO`, `WARNING`, `ERROR`, `CRITICAL`) and tailored message types for different service components.\n",
    "You must create 5 structured logs here are some examples:\n",
    "\n",
    "    sqldb\n",
    "    ui\n",
    "    frontend.js\n",
    "    backend.js\n",
    "    frontend.flask\n",
    "    backend.flask\n",
    "\n",
    "You may use chat GPT to create sample outputs NOT THE LOGS. IE:\n",
    "\n",
    "    System failure\n",
    "    Database corruption\n",
    "    Disk failure detected\n",
    "    Database corruption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9ba30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:31:17,258 - CRITICAL - Replication lag detected\n",
      "2025-04-27 21:31:17,259 - CRITICAL - Index optimization completed\n",
      "2025-04-27 21:31:17,259 - CRITICAL - High CPU usage detected\n",
      "2025-04-27 21:31:17,260 - INFO - Critical: Database server crashed\n",
      "2025-04-27 21:31:17,260 - CRITICAL - Database initialized successfully\n",
      "2025-04-27 21:31:17,261 - INFO - Critical: Database server crashed\n",
      "2025-04-27 21:31:17,261 - INFO - Critical: Database server crashed\n",
      "2025-04-27 21:31:17,262 - INFO - Replication lag detected\n",
      "2025-04-27 21:31:17,262 - WARNING - Replication lag detected\n",
      "2025-04-27 21:31:17,262 - ERROR - Failed to backup database\n",
      "2025-04-27 21:31:17,263 - ERROR - Button click handler crashed\n",
      "2025-04-27 21:31:17,264 - CRITICAL - High memory usage in UI\n",
      "2025-04-27 21:31:17,265 - ERROR - Theme application failed\n",
      "2025-04-27 21:31:17,266 - WARNING - Critical: UI froze during operation\n",
      "2025-04-27 21:31:17,266 - CRITICAL - Slow rendering of dashboard\n",
      "2025-04-27 21:31:17,267 - WARNING - Critical: UI froze during operation\n",
      "2025-04-27 21:31:17,267 - ERROR - Error: Invalid user input detected\n",
      "2025-04-27 21:31:17,267 - INFO - Responsive layout adjusted\n",
      "2025-04-27 21:31:17,268 - WARNING - Theme application failed\n",
      "2025-04-27 21:31:17,268 - ERROR - Accessibility check failed\n",
      "2025-04-27 21:31:17,269 - INFO - API response time exceeded threshold\n",
      "2025-04-27 21:31:17,270 - INFO - Client-side cache cleared\n",
      "2025-04-27 21:31:17,270 - CRITICAL - Client-side cache cleared\n",
      "2025-04-27 21:31:17,271 - INFO - Failed to fetch resource\n",
      "2025-04-27 21:31:17,272 - ERROR - API response time exceeded threshold\n",
      "2025-04-27 21:31:17,273 - ERROR - Error: Uncaught exception in event handler\n",
      "2025-04-27 21:31:17,273 - CRITICAL - Error: Uncaught exception in event handler\n",
      "2025-04-27 21:31:17,274 - CRITICAL - Client-side cache cleared\n",
      "2025-04-27 21:31:17,274 - CRITICAL - Bundle size too large\n",
      "2025-04-27 21:31:17,275 - WARNING - API response time exceeded threshold\n",
      "2025-04-27 21:31:17,276 - WARNING - Request processing delayed\n",
      "2025-04-27 21:31:17,277 - ERROR - Critical: Backend service down\n",
      "2025-04-27 21:31:17,278 - CRITICAL - Rate limit exceeded\n",
      "2025-04-27 21:31:17,279 - CRITICAL - Session cleanup completed\n",
      "2025-04-27 21:31:17,280 - ERROR - Error: Database query failed\n",
      "2025-04-27 21:31:17,280 - INFO - High disk I/O detected\n",
      "2025-04-27 21:31:17,281 - INFO - Rate limit exceeded\n",
      "2025-04-27 21:31:17,281 - CRITICAL - Request processing delayed\n",
      "2025-04-27 21:31:17,281 - WARNING - Session cleanup completed\n",
      "2025-04-27 21:31:17,282 - CRITICAL - Request processing delayed\n",
      "2025-04-27 21:31:17,284 - INFO - Request routing misconfigured\n",
      "2025-04-27 21:31:17,285 - WARNING - Request routing misconfigured\n",
      "2025-04-27 21:31:17,286 - INFO - Failed to load middleware\n",
      "2025-04-27 21:31:17,286 - CRITICAL - Request routing misconfigured\n",
      "2025-04-27 21:31:17,287 - INFO - Request routing misconfigured\n",
      "2025-04-27 21:31:17,287 - WARNING - Request routing misconfigured\n",
      "2025-04-27 21:31:17,287 - WARNING - High network latency detected\n",
      "2025-04-27 21:31:17,288 - CRITICAL - Template rendering took too long\n",
      "2025-04-27 21:31:17,288 - CRITICAL - Flask frontend server running\n",
      "2025-04-27 21:31:17,289 - CRITICAL - Flask frontend server running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated log file: Logs/sqldb.log\n",
      "Generated log file: Logs/ui.log\n",
      "Generated log file: Logs/frontend.js.log\n",
      "Generated log file: Logs/backend.js.log\n",
      "Generated log file: Logs/frontend.flask.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Define log levels and service-specific messages\n",
    "LOG_LEVELS = ['INFO', 'WARNING', 'ERROR', 'CRITICAL']\n",
    "SERVICE_MESSAGES = {\n",
    "    'sqldb': [\n",
    "        \"Database initialized successfully\",\n",
    "        \"Query execution took too long\",\n",
    "        \"Database corruption detected in table\",\n",
    "        \"Critical: Database server crashed\",\n",
    "        \"Connection pool exhausted\",\n",
    "        \"Index optimization completed\",\n",
    "        \"Failed to backup database\",\n",
    "        \"High CPU usage detected\",\n",
    "        \"Transaction log full\",\n",
    "        \"Replication lag detected\"\n",
    "    ],\n",
    "    'ui': [\n",
    "        \"User interface loaded successfully\",\n",
    "        \"Slow rendering of dashboard\",\n",
    "        \"Error: Invalid user input detected\",\n",
    "        \"Critical: UI froze during operation\",\n",
    "        \"Theme application failed\",\n",
    "        \"Responsive layout adjusted\",\n",
    "        \"Button click handler crashed\",\n",
    "        \"High memory usage in UI\",\n",
    "        \"Session timeout error\",\n",
    "        \"Accessibility check failed\"\n",
    "    ],\n",
    "    'frontend.js': [\n",
    "        \"JavaScript frontend initialized\",\n",
    "        \"API response time exceeded threshold\",\n",
    "        \"Error: Uncaught exception in event handler\",\n",
    "        \"Critical: Frontend script halted\",\n",
    "        \"Bundle size too large\",\n",
    "        \"Client-side cache cleared\",\n",
    "        \"Failed to fetch resource\",\n",
    "        \"JavaScript memory leak detected\",\n",
    "        \"Event listener registered\",\n",
    "        \"Cross-browser compatibility issue\"\n",
    "    ],\n",
    "    'backend.js': [\n",
    "        \"Backend server started\",\n",
    "        \"Request processing delayed\",\n",
    "        \"Error: Database query failed\",\n",
    "        \"Critical: Backend service down\",\n",
    "        \"Authentication module loaded\",\n",
    "        \"Rate limit exceeded\",\n",
    "        \"Failed to connect to message queue\",\n",
    "        \"High disk I/O detected\",\n",
    "        \"Session cleanup completed\",\n",
    "        \"Security token validation failed\"\n",
    "    ],\n",
    "    'frontend.flask': [\n",
    "        \"Flask frontend server running\",\n",
    "        \"Template rendering took too long\",\n",
    "        \"Error: 404 page not found\",\n",
    "        \"Critical: Flask app crashed\",\n",
    "        \"Static file served successfully\",\n",
    "        \"Request routing misconfigured\",\n",
    "        \"Failed to load middleware\",\n",
    "        \"High network latency detected\",\n",
    "        \"Session data initialized\",\n",
    "        \"CSRF token missing\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def setup_logging(service_name, log_dir=\"Logs\"):\n",
    "    \"\"\"Set up logging for a specific service.\"\"\"\n",
    "    log_dir = Path(log_dir)\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    log_file = log_dir / f\"{service_name}.log\"\n",
    "    \n",
    "    # Configure logging\n",
    "    logger = logging.getLogger(service_name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.handlers = []  # Clear any existing handlers\n",
    "    \n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(log_file, mode='w')\n",
    "    formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def generate_log_entries(logger, service_name, num_entries=10):\n",
    "    \"\"\"Generate random log entries for a service.\"\"\"\n",
    "    messages = SERVICE_MESSAGES[service_name]\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(num_entries):\n",
    "        level = random.choice(LOG_LEVELS)\n",
    "        message = random.choice(messages)\n",
    "        log_time = start_time + timedelta(minutes=i)\n",
    "        \n",
    "        # Log the message with the appropriate level\n",
    "        if level == 'INFO':\n",
    "            logger.info(message)\n",
    "        elif level == 'WARNING':\n",
    "            logger.warning(message)\n",
    "        elif level == 'ERROR':\n",
    "            logger.error(message)\n",
    "        elif level == 'CRITICAL':\n",
    "            logger.critical(message)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Generate log files for specified services.\"\"\"\n",
    "    services = ['sqldb', 'ui', 'frontend.js', 'backend.js', 'frontend.flask']\n",
    "    \n",
    "    for service in services:\n",
    "        logger = setup_logging(service)\n",
    "        generate_log_entries(logger, service)\n",
    "        print(f\"Generated log file: Logs/{service}.log\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# don't forget to upload it with your submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5255ab",
   "metadata": {},
   "source": [
    "\n",
    "### Example Log Format\n",
    "\n",
    "You will work with logs that follow this simplified structure:\n",
    "\n",
    "```\n",
    "2025-04-11 23:20:36,913 | my_app | INFO | Request completed\n",
    "2025-04-11 23:20:36,914 | my_app.utils | ERROR | Unhandled exception\n",
    "2025-04-11 23:20:36,914 | my_app.utils.db | CRITICAL | Disk failure detected\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3659dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:31:17,308 - INFO - Index optimization completed\n",
      "2025-04-27 21:31:17,310 - WARNING - Transaction log full\n",
      "2025-04-27 21:31:17,311 - CRITICAL - Index optimization completed\n",
      "2025-04-27 21:31:17,312 - CRITICAL - Failed to backup database\n",
      "2025-04-27 21:31:17,313 - CRITICAL - Disk failure detected\n",
      "2025-04-27 21:31:17,315 - CRITICAL - Index optimization completed\n",
      "2025-04-27 21:31:17,317 - ERROR - Database initialized successfully\n",
      "2025-04-27 21:31:17,318 - ERROR - Index optimization completed\n",
      "2025-04-27 21:31:17,319 - ERROR - Connection pool exhausted\n",
      "2025-04-27 21:31:17,319 - ERROR - Disk failure detected\n",
      "2025-04-27 21:31:17,321 - ERROR - User interface loaded successfully\n",
      "2025-04-27 21:31:17,322 - WARNING - Slow rendering of dashboard\n",
      "2025-04-27 21:31:17,322 - WARNING - High memory usage in UI\n",
      "2025-04-27 21:31:17,322 - WARNING - Responsive layout adjusted\n",
      "2025-04-27 21:31:17,323 - INFO - Responsive layout adjusted\n",
      "2025-04-27 21:31:17,323 - WARNING - User interface loaded successfully\n",
      "2025-04-27 21:31:17,324 - CRITICAL - User interface loaded successfully\n",
      "2025-04-27 21:31:17,324 - ERROR - High memory usage in UI\n",
      "2025-04-27 21:31:17,324 - ERROR - Invalid user input detected\n",
      "2025-04-27 21:31:17,325 - WARNING - System failure: UI froze\n",
      "2025-04-27 21:31:17,327 - WARNING - JavaScript frontend initialized\n",
      "2025-04-27 21:31:17,328 - ERROR - Uncaught exception in event handler\n",
      "2025-04-27 21:31:17,329 - CRITICAL - Cross-browser compatibility issue\n",
      "2025-04-27 21:31:17,329 - WARNING - Client-side cache cleared\n",
      "2025-04-27 21:31:17,330 - INFO - Uncaught exception in event handler\n",
      "2025-04-27 21:31:17,330 - ERROR - Failed to fetch resource\n",
      "2025-04-27 21:31:17,330 - CRITICAL - Client-side cache cleared\n",
      "2025-04-27 21:31:17,331 - CRITICAL - Bundle size too large\n",
      "2025-04-27 21:31:17,331 - ERROR - Event listener registered\n",
      "2025-04-27 21:31:17,332 - WARNING - Event listener registered\n",
      "2025-04-27 21:31:17,333 - CRITICAL - Authentication module loaded\n",
      "2025-04-27 21:31:17,333 - ERROR - Request processing delayed\n",
      "2025-04-27 21:31:17,334 - WARNING - Session cleanup completed\n",
      "2025-04-27 21:31:17,334 - CRITICAL - High disk I/O detected\n",
      "2025-04-27 21:31:17,335 - INFO - Database query failed\n",
      "2025-04-27 21:31:17,335 - ERROR - System failure: Backend service down\n",
      "2025-04-27 21:31:17,335 - WARNING - Security token validation failed\n",
      "2025-04-27 21:31:17,336 - WARNING - Request processing delayed\n",
      "2025-04-27 21:31:17,336 - CRITICAL - Failed to connect to message queue\n",
      "2025-04-27 21:31:17,336 - INFO - Session cleanup completed\n",
      "2025-04-27 21:31:17,337 - ERROR - Flask frontend server running\n",
      "2025-04-27 21:31:17,338 - ERROR - System failure: Flask app crashed\n",
      "2025-04-27 21:31:17,338 - ERROR - Session data initialized\n",
      "2025-04-27 21:31:17,338 - WARNING - Failed to load middleware\n",
      "2025-04-27 21:31:17,339 - ERROR - System failure: Flask app crashed\n",
      "2025-04-27 21:31:17,339 - WARNING - Request routing misconfigured\n",
      "2025-04-27 21:31:17,339 - INFO - System failure: Flask app crashed\n",
      "2025-04-27 21:31:17,339 - INFO - Request routing misconfigured\n",
      "2025-04-27 21:31:17,340 - WARNING - Session data initialized\n",
      "2025-04-27 21:31:17,341 - WARNING - Request routing misconfigured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated log file: Logs/sqldb.log\n",
      "Generated log file: Logs/ui.log\n",
      "Generated log file: Logs/frontend.js.log\n",
      "Generated log file: Logs/backend.js.log\n",
      "Generated log file: Logs/frontend.flask.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Define log levels and service-specific messages\n",
    "LOG_LEVELS = ['INFO', 'WARNING', 'ERROR', 'CRITICAL']\n",
    "SERVICE_MESSAGES = {\n",
    "    'sqldb': [\n",
    "        \"Database initialized successfully\",\n",
    "        \"Query execution took too long\",\n",
    "        \"Database corruption detected in table\",\n",
    "        \"Disk failure detected\",\n",
    "        \"Connection pool exhausted\",\n",
    "        \"Index optimization completed\",\n",
    "        \"Failed to backup database\",\n",
    "        \"High CPU usage detected\",\n",
    "        \"Transaction log full\",\n",
    "        \"Replication lag detected\"\n",
    "    ],\n",
    "    'ui': [\n",
    "        \"User interface loaded successfully\",\n",
    "        \"Slow rendering of dashboard\",\n",
    "        \"Invalid user input detected\",\n",
    "        \"System failure: UI froze\",\n",
    "        \"Theme application failed\",\n",
    "        \"Responsive layout adjusted\",\n",
    "        \"Button click handler crashed\",\n",
    "        \"High memory usage in UI\",\n",
    "        \"Session timeout error\",\n",
    "        \"Accessibility check failed\"\n",
    "    ],\n",
    "    'frontend.js': [\n",
    "        \"JavaScript frontend initialized\",\n",
    "        \"API response time exceeded threshold\",\n",
    "        \"Uncaught exception in event handler\",\n",
    "        \"System failure: Frontend script halted\",\n",
    "        \"Bundle size too large\",\n",
    "        \"Client-side cache cleared\",\n",
    "        \"Failed to fetch resource\",\n",
    "        \"JavaScript memory leak detected\",\n",
    "        \"Event listener registered\",\n",
    "        \"Cross-browser compatibility issue\"\n",
    "    ],\n",
    "    'backend.js': [\n",
    "        \"Backend server started\",\n",
    "        \"Request processing delayed\",\n",
    "        \"Database query failed\",\n",
    "        \"System failure: Backend service down\",\n",
    "        \"Authentication module loaded\",\n",
    "        \"Rate limit exceeded\",\n",
    "        \"Failed to connect to message queue\",\n",
    "        \"High disk I/O detected\",\n",
    "        \"Session cleanup completed\",\n",
    "        \"Security token validation failed\"\n",
    "    ],\n",
    "    'frontend.flask': [\n",
    "        \"Flask frontend server running\",\n",
    "        \"Template rendering took too long\",\n",
    "        \"404 page not found\",\n",
    "        \"System failure: Flask app crashed\",\n",
    "        \"Static file served successfully\",\n",
    "        \"Request routing misconfigured\",\n",
    "        \"Failed to load middleware\",\n",
    "        \"High network latency detected\",\n",
    "        \"Session data initialized\",\n",
    "        \"CSRF token missing\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def setup_logging(service_name, log_dir=\"Logs\"):\n",
    "    \"\"\"Set up logging for a specific service with custom format.\"\"\"\n",
    "    log_dir = Path(log_dir)\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    log_file = log_dir / f\"{service_name}.log\"\n",
    "    \n",
    "    # Configure logging\n",
    "    logger = logging.getLogger(service_name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.handlers = []  # Clear existing handlers\n",
    "    \n",
    "    # File handler with custom formatter\n",
    "    file_handler = logging.FileHandler(log_file, mode='w')\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s,%(msecs)03d | %(name)s | %(levelname)s | %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def generate_log_entries(logger, service_name, num_entries=10):\n",
    "    \"\"\"Generate random log entries for a service.\"\"\"\n",
    "    messages = SERVICE_MESSAGES[service_name]\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(num_entries):\n",
    "        level = random.choice(LOG_LEVELS)\n",
    "        message = random.choice(messages)\n",
    "        log_time = start_time + timedelta(seconds=i)  # Increment by seconds for variety\n",
    "        \n",
    "        # Log with the appropriate level\n",
    "        if level == 'INFO':\n",
    "            logger.info(message)\n",
    "        elif level == 'WARNING':\n",
    "            logger.warning(message)\n",
    "        elif level == 'ERROR':\n",
    "            logger.error(message)\n",
    "        elif level == 'CRITICAL':\n",
    "            logger.critical(message)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Generate log files for specified services.\"\"\"\n",
    "    services = ['sqldb', 'ui', 'frontend.js', 'backend.js', 'frontend.flask']\n",
    "    \n",
    "    for service in services:\n",
    "        logger = setup_logging(service)\n",
    "        generate_log_entries(logger, service)\n",
    "        print(f\"Generated log file: Logs/{service}.log\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f6e84",
   "metadata": {},
   "source": [
    "## Part 2: Logging the Log File (40%)\n",
    "    New File\n",
    "### Part 2a: Read the Log File (see lab 7) (10%)\n",
    "\n",
    "\n",
    "Write a function to read the contents of a log file into a list of lines. Handle file errors gracefully.\n",
    "\n",
    "### Part 2b: Parse Log Lines (see code below if you get stuck) (10%)\n",
    "\n",
    "Use a regular expression to extract:\n",
    "- Timestamp\n",
    "- Log name\n",
    "- Log level\n",
    "- Message\n",
    "\n",
    "### Part 2c: Count Log Levels (20%)\n",
    "\n",
    "Create a function to count how many times each log level appears. Store the results in a dictionary. Then output it as a Json File\n",
    "You may pick your own format but here is an example. \n",
    "```python\n",
    "{\n",
    "    \"INFO\": \n",
    "    {\n",
    "        \"Request completed\": 42, \n",
    "        \"Heartbeat OK\": 7\n",
    "    }\n",
    "\n",
    "    \"WARNING\":\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bc631f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:31:17,355 - INFO - Read 10 lines from Logs/ui.log\n",
      "2025-04-27 21:31:17,357 - INFO - Read 10 lines from Logs/frontend.js.log\n",
      "2025-04-27 21:31:17,358 - INFO - Read 10 lines from Logs/frontend.flask.log\n",
      "2025-04-27 21:31:17,358 - INFO - Read 10 lines from Logs/backend.js.log\n",
      "2025-04-27 21:31:17,359 - INFO - Read 10 lines from Logs/sqldb.log\n",
      "2025-04-27 21:31:17,360 - INFO - Read 30 lines from Logs/systemConfiguredLogger.log\n",
      "2025-04-27 21:31:17,360 - WARNING - Invalid log line: {\n",
      "2025-04-27 21:31:17,360 - WARNING - Invalid log line: \"timestamp\": \"2025-04-28T03:17:21.312387Z\",\n",
      "2025-04-27 21:31:17,360 - WARNING - Invalid log line: \"component\": \"systemConfiguredLogger\",\n",
      "2025-04-27 21:31:17,360 - WARNING - Invalid log line: \"level\": \"CRITICAL\",\n",
      "2025-04-27 21:31:17,361 - WARNING - Invalid log line: \"message\": \"The Sun is Bright\"\n",
      "2025-04-27 21:31:17,361 - WARNING - Invalid log line: }\n",
      "2025-04-27 21:31:17,361 - WARNING - Invalid log line: {\n",
      "2025-04-27 21:31:17,361 - WARNING - Invalid log line: \"timestamp\": \"2025-04-28T03:17:21.312803Z\",\n",
      "2025-04-27 21:31:17,362 - WARNING - Invalid log line: \"component\": \"systemConfiguredLogger\",\n",
      "2025-04-27 21:31:17,362 - WARNING - Invalid log line: \"level\": \"CRITICAL\",\n",
      "2025-04-27 21:31:17,362 - WARNING - Invalid log line: \"message\": \"storage space is low\"\n",
      "2025-04-27 21:31:17,363 - WARNING - Invalid log line: }\n",
      "2025-04-27 21:31:17,363 - WARNING - Invalid log line: {\n",
      "2025-04-27 21:31:17,363 - WARNING - Invalid log line: \"timestamp\": \"2025-04-28T03:17:21.312885Z\",\n",
      "2025-04-27 21:31:17,363 - WARNING - Invalid log line: \"component\": \"systemConfiguredLogger\",\n",
      "2025-04-27 21:31:17,364 - WARNING - Invalid log line: \"level\": \"ERROR\",\n",
      "2025-04-27 21:31:17,364 - WARNING - Invalid log line: \"message\": \"file not found create new file\"\n",
      "2025-04-27 21:31:17,364 - WARNING - Invalid log line: }\n",
      "2025-04-27 21:31:17,364 - WARNING - Invalid log line: {\n",
      "2025-04-27 21:31:17,365 - WARNING - Invalid log line: \"timestamp\": \"2025-04-28T03:17:21.313132Z\",\n",
      "2025-04-27 21:31:17,365 - WARNING - Invalid log line: \"component\": \"systemConfiguredLogger\",\n",
      "2025-04-27 21:31:17,365 - WARNING - Invalid log line: \"level\": \"WARNING\",\n",
      "2025-04-27 21:31:17,365 - WARNING - Invalid log line: \"message\": \"disk space is low\"\n",
      "2025-04-27 21:31:17,365 - WARNING - Invalid log line: }\n",
      "2025-04-27 21:31:17,366 - WARNING - Invalid log line: {\n",
      "2025-04-27 21:31:17,366 - WARNING - Invalid log line: \"timestamp\": \"2025-04-28T03:17:21.313215Z\",\n",
      "2025-04-27 21:31:17,366 - WARNING - Invalid log line: \"component\": \"systemConfiguredLogger\",\n",
      "2025-04-27 21:31:17,366 - WARNING - Invalid log line: \"level\": \"INFO\",\n",
      "2025-04-27 21:31:17,366 - WARNING - Invalid log line: \"message\": \"user logged in with correct password\"\n",
      "2025-04-27 21:31:17,367 - WARNING - Invalid log line: }\n",
      "2025-04-27 21:31:17,367 - INFO - Saved summary to summary.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Log Level Summary:\n",
      "------------------\n",
      "ERROR: 16 entries\n",
      "  - User interface loaded successfully: 1\n",
      "  - High memory usage in UI: 1\n",
      "  - Invalid user input detected: 1\n",
      "  - Uncaught exception in event handler: 1\n",
      "  - Failed to fetch resource: 1\n",
      "  - Event listener registered: 1\n",
      "  - Flask frontend server running: 1\n",
      "  - System failure: Flask app crashed: 2\n",
      "  - Session data initialized: 1\n",
      "  - Request processing delayed: 1\n",
      "  - System failure: Backend service down: 1\n",
      "  - Database initialized successfully: 1\n",
      "  - Index optimization completed: 1\n",
      "  - Connection pool exhausted: 1\n",
      "  - Disk failure detected: 1\n",
      "WARNING: 16 entries\n",
      "  - Slow rendering of dashboard: 1\n",
      "  - High memory usage in UI: 1\n",
      "  - Responsive layout adjusted: 1\n",
      "  - User interface loaded successfully: 1\n",
      "  - System failure: UI froze: 1\n",
      "  - JavaScript frontend initialized: 1\n",
      "  - Client-side cache cleared: 1\n",
      "  - Event listener registered: 1\n",
      "  - Failed to load middleware: 1\n",
      "  - Request routing misconfigured: 2\n",
      "  - Session data initialized: 1\n",
      "  - Session cleanup completed: 1\n",
      "  - Security token validation failed: 1\n",
      "  - Request processing delayed: 1\n",
      "  - Transaction log full: 1\n",
      "INFO: 7 entries\n",
      "  - Responsive layout adjusted: 1\n",
      "  - Uncaught exception in event handler: 1\n",
      "  - System failure: Flask app crashed: 1\n",
      "  - Request routing misconfigured: 1\n",
      "  - Database query failed: 1\n",
      "  - Session cleanup completed: 1\n",
      "  - Index optimization completed: 1\n",
      "CRITICAL: 11 entries\n",
      "  - User interface loaded successfully: 1\n",
      "  - Cross-browser compatibility issue: 1\n",
      "  - Client-side cache cleared: 1\n",
      "  - Bundle size too large: 1\n",
      "  - Authentication module loaded: 1\n",
      "  - High disk I/O detected: 1\n",
      "  - Failed to connect to message queue: 1\n",
      "  - Index optimization completed: 2\n",
      "  - Failed to backup database: 1\n",
      "  - Disk failure detected: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "# Configure logging for debugging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_log_file(file_path):\n",
    "    \"\"\"Part 2a: Read log file into a list of lines, handling errors.\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    lines = []\n",
    "    \n",
    "    if not file_path.is_file():\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        return lines\n",
    "    \n",
    "    try:\n",
    "        with file_path.open('r') as f:\n",
    "            lines = [line.strip() for line in f if line.strip()]\n",
    "        logger.info(f\"Read {len(lines)} lines from {file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading {file_path}: {str(e)}\")\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def parse_log_line(line):\n",
    "    \"\"\"Part 2b: Parse a log line using regex to extract timestamp, log name, level, and message.\"\"\"\n",
    "    # Regex for format: 2025-04-27 10:00:00,123 | service_name | LEVEL | Message\n",
    "    pattern = r'^(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}) \\| ([^|]+) \\| (INFO|WARNING|ERROR|CRITICAL) \\| (.+)$'\n",
    "    match = re.match(pattern, line)\n",
    "    \n",
    "    if match:\n",
    "        timestamp, log_name, level, message = match.groups()\n",
    "        return {\n",
    "            'timestamp': timestamp,\n",
    "            'log_name': log_name.strip(),\n",
    "            'level': level,\n",
    "            'message': message.strip()\n",
    "        }\n",
    "    logger.warning(f\"Invalid log line: {line}\")\n",
    "    return None\n",
    "\n",
    "def count_log_levels(log_files):\n",
    "    \"\"\"Part 2c: Count log levels and messages, return a dictionary.\"\"\"\n",
    "    level_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for log_file in log_files:\n",
    "        lines = read_log_file(log_file)\n",
    "        for line in lines:\n",
    "            parsed = parse_log_line(line)\n",
    "            if parsed:\n",
    "                level = parsed['level']\n",
    "                message = parsed['message']\n",
    "                level_counts[level][message] += 1\n",
    "    \n",
    "    # Convert defaultdict to regular dict for JSON serialization\n",
    "    return {level: dict(messages) for level, messages in level_counts.items()}\n",
    "\n",
    "def save_summary(summary, output_file=\"summary.json\"):\n",
    "    \"\"\"Part 2c: Save the summary dictionary as a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "        logger.info(f\"Saved summary to {output_file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving summary to {output_file}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Process all log files in Logs/ and generate summary.\"\"\"\n",
    "    log_dir = Path(\"Logs\")\n",
    "    if not log_dir.exists():\n",
    "        logger.error(\"Logs directory not found. Run generate_logs.py first.\")\n",
    "        return\n",
    "    \n",
    "    # Get all .log files\n",
    "    log_files = list(log_dir.glob(\"*.log\"))\n",
    "    if not log_files:\n",
    "        logger.error(\"No log files found in Logs directory.\")\n",
    "        return\n",
    "    \n",
    "    # Count log levels and messages\n",
    "    summary = count_log_levels(log_files)\n",
    "    \n",
    "    # Save to JSON\n",
    "    save_summary(summary)\n",
    "    \n",
    "    # Print summary for verification\n",
    "    print(\"\\nLog Level Summary:\")\n",
    "    print(\"------------------\")\n",
    "    for level, messages in summary.items():\n",
    "        total = sum(messages.values())\n",
    "        print(f\"{level}: {total} entries\")\n",
    "        for message, count in messages.items():\n",
    "            print(f\"  - {message}: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "# Paste your python file here don't for get to upload it with your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5f8a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045c30f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Generate Summary Report (40%)\n",
    "    New File\n",
    "### Step 3a (20%):\n",
    " Develop a function that continuously monitors your JSON file(s) and will print a real-time summary of log activity. It should keep count of the messages grouped by log level (INFO, WARNING, ERROR, CRITICAL) and display only the critical messages. (I.e. If new data comes in the summary will change and a new critical message will be printed)\n",
    " - note: do not reprocess the entire file on each update.  \n",
    "\n",
    "### Step 3a: Use a Matplotlib (Lecture 10) (20%)\n",
    "Develop a function that continuously monitors your JSON file(s) and will graph in real-time a bar or pie plot of each of the errors.  (a graph for each log level). \n",
    "- The graph should show the distribution of log messages by level  (INFO, WARNING, ERROR, CRITICAL)  \n",
    "\n",
    "\n",
    "### Critical notes:\n",
    "- Your code mus use Daemon Threads (Lecture 14)\n",
    "- 3a and 3b do not need to run at the same time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bea4429f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Configure logging\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load JSON file, handling errors.\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.error(f\"JSON file not found: {file_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with file_path.open('r') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_level_counts(summary):\n",
    "    \"\"\"Calculate total message counts per log level.\"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    if summary:\n",
    "        for level, messages in summary.items():\n",
    "            counts[level] = sum(messages.values())\n",
    "    return counts\n",
    "\n",
    "def monitor_summary_report(json_file, interval=1):\n",
    "    \"\"\"Step 3a: Monitor JSON file and print real-time summary with CRITICAL messages.\"\"\"\n",
    "    json_file = Path(json_file)\n",
    "    last_mtime = 0\n",
    "    last_critical = {}\n",
    "    \n",
    "    def print_summary():\n",
    "        nonlocal last_mtime, last_critical\n",
    "        while True:\n",
    "            try:\n",
    "                # Check if file has been modified\n",
    "                current_mtime = json_file.stat().st_mtime\n",
    "                if current_mtime > last_mtime:\n",
    "                    summary = load_json_file(json_file)\n",
    "                    if summary:\n",
    "                        # Update last modification time\n",
    "                        last_mtime = current_mtime\n",
    "                        \n",
    "                        # Get total counts per level\n",
    "                        counts = get_level_counts(summary)\n",
    "                        \n",
    "                        # Print summary\n",
    "                        print(\"\\nReal-Time Log Summary:\")\n",
    "                        print(\"----------------------\")\n",
    "                        for level in ['INFO', 'WARNING', 'ERROR', 'CRITICAL']:\n",
    "                            count = counts.get(level, 0)\n",
    "                            print(f\"{level}: {count} messages\")\n",
    "                        \n",
    "                        # Print new CRITICAL messages\n",
    "                        critical_messages = summary.get('CRITICAL', {})\n",
    "                        for message, count in critical_messages.items():\n",
    "                            last_count = last_critical.get(message, 0)\n",
    "                            if count > last_count:\n",
    "                                print(f\"New CRITICAL: {message} (Count: {count})\")\n",
    "                        last_critical = critical_messages.copy()\n",
    "                    \n",
    "                    logger.info(f\"Processed update to {json_file}\")\n",
    "                time.sleep(interval)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in monitor_summary_report: {str(e)}\")\n",
    "                time.sleep(interval)\n",
    "    \n",
    "    # Run in a daemon thread\n",
    "    thread = threading.Thread(target=print_summary, daemon=True)\n",
    "    thread.start()\n",
    "    return thread\n",
    "\n",
    "def monitor_plot(json_file, interval=1):\n",
    "    \"\"\"Step 3b: Monitor JSON file and update a real-time bar plot of log levels.\"\"\"\n",
    "    json_file = Path(json_file)\n",
    "    last_mtime = 0\n",
    "    \n",
    "    # Set up Matplotlib for non-blocking display\n",
    "    plt.ion()  # Interactive mode\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    def update_plot():\n",
    "        nonlocal last_mtime\n",
    "        while True:\n",
    "            try:\n",
    "                # Check if file has been modified\n",
    "                current_mtime = json_file.stat().st_mtime\n",
    "                if current_mtime > last_mtime:\n",
    "                    summary = load_json_file(json_file)\n",
    "                    if summary:\n",
    "                        # Update last modification time\n",
    "                        last_mtime = current_mtime\n",
    "                        \n",
    "                        # Get counts\n",
    "                        counts = get_level_counts(summary)\n",
    "                        levels = ['INFO', 'WARNING', 'ERROR', 'CRITICAL']\n",
    "                        values = [counts.get(level, 0) for level in levels]\n",
    "                        \n",
    "                        # Update plot\n",
    "                        ax.clear()\n",
    "                        ax.bar(levels, values, color=['blue', 'yellow', 'orange', 'red'])\n",
    "                        ax.set_title('Real-Time Log Level Distribution')\n",
    "                        ax.set_xlabel('Log Level')\n",
    "                        ax.set_ylabel('Message Count')\n",
    "                        ax.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        fig.canvas.draw()\n",
    "                        fig.canvas.flush_events()\n",
    "                        logger.info(f\"Updated plot for {json_file}\")\n",
    "                time.sleep(interval)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in monitor_plot: {str(e)}\")\n",
    "                time.sleep(interval)\n",
    "    \n",
    "    # Run in a daemon thread\n",
    "    thread = threading.Thread(target=update_plot, daemon=True)\n",
    "    thread.start()\n",
    "    return thread\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the monitoring script based on command-line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Monitor log summary JSON file\")\n",
    "    parser.add_argument(\n",
    "        '--mode',\n",
    "        choices=['summary', 'plot'],\n",
    "        required=True,\n",
    "        help='Mode to run: \"summary\" for text summary, \"plot\" for graph'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--json-file',\n",
    "        default='summary.json',\n",
    "        help='Path to the summary JSON file'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--interval',\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help='Monitoring interval in seconds'\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    json_file = Path(args.json_file)\n",
    "    if not json_file.is_file():\n",
    "        logger.error(f\"JSON file not found: {json_file}. Run process_logs.py first.\")\n",
    "        return\n",
    "    \n",
    "    if args.mode == 'summary':\n",
    "        logger.info(\"Starting real-time summary monitor...\")\n",
    "        thread = monitor_summary_report(args.json_file, args.interval)\n",
    "    else:\n",
    "        logger.info(\"Starting real-time plot monitor...\")\n",
    "        thread = monitor_plot(args.json_file, args.interval)\n",
    "    \n",
    "    # Keep the main thread alive\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Stopping monitor...\")\n",
    "        plt.close('all')  # Close Matplotlib windows if open\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26eb058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created summary.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_log(file_path):\n",
    "    \"\"\"Read log file into lines.\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        print(f\"Error: {file_path} not found\")\n",
    "        return []\n",
    "    try:\n",
    "        with file_path.open('r') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def parse_line(line):\n",
    "    \"\"\"Parse log line with regex.\"\"\"\n",
    "    pattern = r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3} \\| ([^|]+) \\| (INFO|WARNING|ERROR|CRITICAL) \\| (.+)$'\n",
    "    match = re.match(pattern, line)\n",
    "    return match.groups() if match else None\n",
    "\n",
    "def count_levels(log_files):\n",
    "    \"\"\"Count log levels and messages.\"\"\"\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "    for file in log_files:\n",
    "        for line in read_log(file):\n",
    "            parsed = parse_line(line)\n",
    "            if parsed:\n",
    "                _, level, message = parsed\n",
    "                counts[level][message] += 1\n",
    "    return {level: dict(messages) for level, messages in counts.items()}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Process logs and save summary.\"\"\"\n",
    "    log_dir = Path(\"Logs\")\n",
    "    if not log_dir.exists():\n",
    "        print(\"Error: Logs directory not found. Run generate_logs.py first.\")\n",
    "        return\n",
    "    \n",
    "    log_files = list(log_dir.glob(\"*.log\"))\n",
    "    if not log_files:\n",
    "        print(\"Error: No log files found in Logs.\")\n",
    "        return\n",
    "    \n",
    "    summary = count_levels(log_files)\n",
    "    with open(\"summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "    print(\"Created summary.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "# Here is a sample regex that parses a log file and extracts relevant information. \n",
    "# you will need to modify it. Review Lecture 11\n",
    "import re\n",
    "\n",
    "def parse_log_line(line):\n",
    "    pattern = r\"^(.*?)\\s\\|\\s(\\w+)\\s\\|\\s(\\w+)\\s\\|\\s(.*)$\"\n",
    "    match = re.match(pattern, line)\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
